{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f699a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e6408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import BilingualDataset, causal_mask \n",
    "from model import build_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df431fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import get_weights_file_path, get_config\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0a33b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454879ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3021b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070b7c4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ea2bca",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "    \n",
    "    # Precompute the encoder output and reuse it for every token we get from the decoder\\\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    \n",
    "    # Initialize the decoder inpt with the sos token\n",
    "    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n",
    "    \n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "        \n",
    "        # Build mask for the target (decoder input)\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "        \n",
    "        # Calculate the output fo the decoder\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "        \n",
    "        # Get the next token\n",
    "        prob = model.project(out[:,-1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat([decoder_input, torch.empty(1,1).type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "        \n",
    "    return decoder_input.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50623e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    \n",
    "    # Size of the control window\n",
    "    console_width = 80\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)\n",
    "            \n",
    "            assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\"\n",
    "            \n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "            \n",
    "            source_text = batch['src_text'][0]\n",
    "            target_text = batch['tgt_text'][0]\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "            \n",
    "            # Print to the console\n",
    "#             print_msg*('-'*console_width)\n",
    "            print_msg(f'SOURCE: {source_text}')\n",
    "            print_msg(f'TARGET: {target_text}')\n",
    "            print_msg(f'PREDICTED: {model_out_text}')\n",
    "            \n",
    "            if count == num_examples:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86321299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang] # any one language... like, works bothways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ffa85a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_or_build_transformer(config, ds, lang):\n",
    "    # config['tokenizer_file'] = '../tokenizers/tokenizer_{0}.json' \n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    \n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token='[UNK]')) # UNK means unknown, when tokenizing the input and the word not found, it maps to the number corresponding to UNK\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "        \n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "        \n",
    "    return tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d26f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "def get_ds(config):\n",
    "\n",
    "    # ================== MODIFICATION START ==================\n",
    "    # Instead of loading opus_books from HuggingFace (needs internet),\n",
    "    # we load LOCAL parallel text files\n",
    "\n",
    "    src_path = \"train.en\"\n",
    "    tgt_path = \"train.fr\"\n",
    "\n",
    "    with open(src_path, encoding=\"utf-8\") as f:\n",
    "        src_sentences = f.read().splitlines()\n",
    "\n",
    "    with open(tgt_path, encoding=\"utf-8\") as f:\n",
    "        tgt_sentences = f.read().splitlines()\n",
    "\n",
    "    assert len(src_sentences) == len(tgt_sentences)\n",
    "\n",
    "    # Creating a HuggingFace-style Dataset so rest of the code remains SAME\n",
    "    ds_raw = Dataset.from_dict({\n",
    "        \"translation\": [\n",
    "            {\n",
    "                config[\"lang_src\"]: src,\n",
    "                config[\"lang_tgt\"]: tgt\n",
    "            }\n",
    "            for src, tgt in zip(src_sentences, tgt_sentences)\n",
    "        ]\n",
    "    })\n",
    "    # ================== MODIFICATION END ==================\n",
    "\n",
    "    # Build tokenizers (UNCHANGED)\n",
    "    tokenizer_src = get_or_build_transformer(config, ds_raw, config[\"lang_src\"])\n",
    "    tokenizer_tgt = get_or_build_transformer(config, ds_raw, config[\"lang_tgt\"])\n",
    "\n",
    "    # splitting datasets for train(90%) and validation(10%) (UNCHANGED)\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "    train_ds = BilingualDataset(\n",
    "        train_ds_raw, tokenizer_src, tokenizer_tgt,\n",
    "        config['lang_src'], config['lang_tgt'], config['seq_len']\n",
    "    )\n",
    "\n",
    "    val_ds = BilingualDataset(\n",
    "        val_ds_raw, tokenizer_src, tokenizer_tgt,\n",
    "        config['lang_src'], config['lang_tgt'], config['seq_len']\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=1,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a504ac51",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae73731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    # Define the device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"using device {device}\")\n",
    "    \n",
    "    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "    \n",
    "    # TensorBoard\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "    \n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    if config['preload']: \n",
    "        model_filename = get_weights_file_path(config, config['preload'])\n",
    "        print(f\"Preloading Model {model_filename}\")\n",
    "        state = torch.load(model_filename, map_location=device)\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "        \n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
    "     \n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        # model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f'Processing epoch: {epoch:02d}')\n",
    "        \n",
    "        for batch in batch_iterator:\n",
    "            \n",
    "            model.train() \n",
    "            \n",
    "            encoder_input = batch['encoder_input'].to(device) # (B, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "            \n",
    "            # Run the tensors through transformer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "            proj_output = model.project(decoder_output) # (B, seq_len, tgt_vocab_size)\n",
    "            \n",
    "            label = batch['label'].to(device) # (B, seq_len) \n",
    "            \n",
    "            # (B, seq_len, tgt_vocab_size) -> (B * seq_len, tgt_vocab_size)\n",
    "            loss = loss_fn(proj_output.reshape(-1, tokenizer_tgt.get_vocab_size()), label.reshape(-1)) # all view been changed to reshape coz view throw err itseems...\n",
    "            batch_iterator.set_postfix({f'loss': f'{loss.item():6.3f}'})\n",
    "            \n",
    "            # Log the loss\n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "            \n",
    "            # Backpropogate the loss\n",
    "            loss.backward()\n",
    "            \n",
    "            # Under the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "#             run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "            \n",
    "            global_step += 1 # used for the tensorboard to keep track of the loss\n",
    "            \n",
    "        # Saving the model at the end of every epochs\n",
    "        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd96a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings('ignore')\n",
    "    config = get_config()\n",
    "    train_model(config)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997c57cd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
